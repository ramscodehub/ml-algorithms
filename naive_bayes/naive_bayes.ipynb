{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx33Zkrv7izY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HSJWTGoAZXQ"
      },
      "source": [
        "#load the data and convert into a pandas DataFrame\n",
        "dataset=load_iris()\n",
        "features=dataset.data\n",
        "target=dataset.target\n",
        "sepal_length=features[:,0]\n",
        "sepal_width=features[:,1]\n",
        "petal_length=features[:,2]\n",
        "petal_width=features[:,3]\n",
        "target_variable=target[:]\n",
        "#make a dictionary with features\n",
        "features={\"sepal length (cm)\":sepal_length,\n",
        "                     \"sepal width (cm)\":sepal_width,\n",
        "                     \"petal length (cm)\":petal_length,\n",
        "                     \"petal width (cm)\":petal_width}\n",
        "#convert the dictionary to a DataFrame\n",
        "df=pd.DataFrame(features)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpdeWJAwHasO"
      },
      "source": [
        "#normalise the feature values\n",
        "X=df.values\n",
        "sc=StandardScaler()\n",
        "normalised=sc.fit_transform(X)\n",
        "df=pd.DataFrame(data=normalised,columns=dataset.feature_names)\n",
        "#adding the target class to the dataframe\n",
        "df[\"target class\"]=target_variable"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEGSrBRNKDq_"
      },
      "source": [
        "#split the data for training and testing purpose\n",
        "x=normalised\n",
        "y=target_variable\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=40,random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCKcAUe1VXrz",
        "outputId": "f379de0b-5db8-4982-c13a-bd796bd283b7"
      },
      "source": [
        "xtrain.shape,ytrain.shape,xtest.shape,ytest.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((110, 4), (110,), (40, 4), (40,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58sc4LB_LreE"
      },
      "source": [
        "*Implementation of Naive-Bayes from scratch*\n",
        "Algorithm:\n",
        "1. Find the posterior probability of given features with each of the classes.\n",
        "        P(y=1|X) is proportional to P(X|y=1)*P(y)\n",
        "        where, P(y=1|X) = posterior probability\n",
        "        P(y) = prior probability\n",
        "        P(X|y=1) = likelihood or class conditional probability\n",
        "2. Take the argmax of all posterior probabilities that will give the index of target classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op887SxPKq2p"
      },
      "source": [
        "class My_Naive_Bayes:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self,x,y):\n",
        "        self.x=x\n",
        "        self.y=y\n",
        "    def calculate_prior_probability(self,label):\n",
        "        \"\"\"Inputs:\n",
        "           label:The target variables\"\"\"\n",
        "        total_examples=self.y.shape[0]\n",
        "        label_examples=np.sum(self.y == label)\n",
        "        return label_examples/total_examples\n",
        "    \n",
        "    def class_conditional_probability(self,feature_column,feature_value,label):\n",
        "        \"\"\"if feature column=petal length,feature value=5 and label=1\n",
        "        this function returns the probability of feature value in feature column when the label is 1\"\"\"\n",
        "        #filter those in x with y==label\n",
        "        xfiltered=self.x[self.y==label]\n",
        "        numerator=np.sum(xfiltered[:,feature_column]==feature_value)\n",
        "        denominator=len(xfiltered)\n",
        "        return numerator/denominator\n",
        "    \n",
        "    def predict_single_example(self,xtest):\n",
        "        \"\"\"Inputs:\n",
        "        xtest: single example with n features\"\"\"\n",
        "        unique_classes=np.unique(self.y)\n",
        "        n_features=self.x.shape[1]\n",
        "        posterior_probabilities=[]\n",
        "        #calculate post.prob for each classs\n",
        "        #post.prob = prior * likelihood\n",
        "        for label in unique_classes:\n",
        "            likelihood=1\n",
        "            for feature in range(n_features):\n",
        "                conditional=self.class_conditional_probability(feature,xtest[feature],label)\n",
        "                likelihood *= conditional\n",
        "            prior=self.calculate_prior_probability(label)\n",
        "            posterior_probabilities.append(prior*likelihood)\n",
        "        return np.argmax(posterior_probabilities)\n",
        "    \n",
        "    def predict(self,nd_array):\n",
        "        result=[]\n",
        "        for point in nd_array:\n",
        "            result.append(self.predict_single_example(point))\n",
        "        return np.array(result)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gflpN7kiWYLI"
      },
      "source": [
        "*Predictions using Naive Bayes from scratch:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFBl0jVvRMVC",
        "outputId": "6384079e-4356-4707-915f-4ac44c62762c"
      },
      "source": [
        "my_model=My_Naive_Bayes()\n",
        "my_model.fit(xtrain,ytrain)\n",
        "pred=my_model.predict(xtest)\n",
        "print(accuracy_score(ytest,pred))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSeh6zi5WiEX"
      },
      "source": [
        "*Predictions using sklearn's Naive Bayes:* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baD0VZLOWP2q",
        "outputId": "badefef4-678a-462d-a391-56e5a7f46f7d"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "sklearn_model=GaussianNB()\n",
        "predicted=sklearn_model.fit(xtrain,ytrain).predict(xtest)\n",
        "print(accuracy_score(ytest,predicted))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}